{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/midhunjmes/midhun-26-02-24/blob/main/Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install openpyxl\n",
        "!pip install presidio_analyzer\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "gP5BWVM0NAh1",
        "outputId": "acd0d8ae-bfbc-4be3-9d72-20a75a8751a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Collecting presidio_analyzer\n",
            "  Downloading presidio_analyzer-2.2.358-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting phonenumbers<9.0.0,>=8.12 (from presidio_analyzer)\n",
            "  Downloading phonenumbers-8.13.55-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from presidio_analyzer) (6.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from presidio_analyzer) (2024.11.6)\n",
            "Requirement already satisfied: spacy!=3.7.0,<4.0.0,>=3.4.4 in /usr/local/lib/python3.11/dist-packages (from presidio_analyzer) (3.8.4)\n",
            "Collecting tldextract (from presidio_analyzer)\n",
            "  Downloading tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.5.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from tldextract->presidio_analyzer) (3.10)\n",
            "Collecting requests-file>=1.4 (from tldextract->presidio_analyzer)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract->presidio_analyzer) (3.18.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.1.2)\n",
            "Downloading presidio_analyzer-2.2.358-py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading phonenumbers-8.13.55-py2.py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.1.3-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Installing collected packages: phonenumbers, requests-file, tldextract, presidio_analyzer\n",
            "Successfully installed phonenumbers-8.13.55 presidio_analyzer-2.2.358 requests-file-2.1.0 tldextract-5.1.3\n",
            "Collecting en-core-web-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install presidio_anonymizer"
      ],
      "metadata": {
        "id": "E0bWKGGpjqzy",
        "outputId": "b0e79906-1323-4820-f93f-dc48100b7947",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting presidio_anonymizer\n",
            "  Downloading presidio_anonymizer-2.2.358-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: cryptography<44.1 in /usr/local/lib/python3.11/dist-packages (from presidio_anonymizer) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<44.1->presidio_anonymizer) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<44.1->presidio_anonymizer) (2.22)\n",
            "Downloading presidio_anonymizer-2.2.358-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: presidio_anonymizer\n",
            "Successfully installed presidio_anonymizer-2.2.358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker"
      ],
      "metadata": {
        "id": "HX0cbevDj_yt",
        "outputId": "29c289ab-f099-4cea-8edc-3cf9d201d637",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading faker-37.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.1)\n",
            "Downloading faker-37.1.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-37.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from presidio_analyzer import AnalyzerEngine\n",
        "\n",
        "# analyzer = AnalyzerEngine()\n",
        "# for recognizer in analyzer.get_recognizers():\n",
        "#     print(recognizer.supported_entities)"
      ],
      "metadata": {
        "id": "clvNKuSWc9QN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from faker import Faker\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import re\n",
        "import openpyxl\n",
        "import json\n",
        "from presidio_analyzer import AnalyzerEngine\n",
        "analyzer = AnalyzerEngine()\n",
        "mapping={}\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "fake=Faker()\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "#function to find out the noun values dominating columns considering it will be a sensitive data if there is so many nouns\n",
        "#------------------------------------------------------------------------------------------------------------------------------\n",
        "def detect_noun(file_path):\n",
        "    if file_path.endswith(\".csv\"):\n",
        "        df = pd.read_csv(file_path, engine=\"python\")\n",
        "    elif file_path.endswith((\".xls\", \".xlsx\")):\n",
        "        df = pd.read_excel(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide a CSV or Excel file.\")\n",
        "\n",
        "    sensitive = []\n",
        "\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype in ['int64', 'float64']:\n",
        "            continue  # Skip purely numeric columns\n",
        "\n",
        "        text_samples = df[col].astype(str).head(5)  # Take first 5 values\n",
        "        noun_count = 0\n",
        "        total_count = 0\n",
        "\n",
        "        for value in text_samples:\n",
        "            if \"%\" in value or value.replace(\".\", \"\").isdigit():\n",
        "                continue  # Skip percentage or number-like values\n",
        "\n",
        "            doc = nlp(value)\n",
        "            for token in doc:\n",
        "                if token.pos_ in ['NOUN', 'PROPN']:\n",
        "                    noun_count += 1\n",
        "                total_count += 1\n",
        "\n",
        "        # Mark column as sensitive only if a significant portion are nouns\n",
        "        if total_count > 0 and (noun_count / total_count) > 0.2:\n",
        "            sensitive.append(col)\n",
        "\n",
        "    return sensitive\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------------------------\n",
        "#finding out the descriptive data columns which may have sensitive data in the form of text\n",
        "#------------------------------------------------------------------------------------------\n",
        "def descriptive_columns(file_path):\n",
        "    # Define keywords to filter out\n",
        "    keywords = [\"description\", \"remarks\", \"notes\", \"comments\", \"observations\", \"details\", \"summary\", \"explanation\",\n",
        "    \"reviews\", \"feedback\", \"testimonials\", \"opinions\", \"assessment\", \"suggestions\", \"experience\",\n",
        "    \"incident_report\", \"case_notes\", \"audit_notes\", \"findings\", \"status_update\", \"history\", \"progress_report\",\n",
        "    \"additional_info\", \"clarifications\", \"justification\", \"annotations\", \"excerpts\", \"statement\", \"explanation_text\"]\n",
        "\n",
        "    # Ensure columns are properly loaded from CSV/Excel\n",
        "    if file_path.endswith(\".csv\"):\n",
        "        df = pd.read_csv(file_path, nrows=1)  # Read only header\n",
        "    elif file_path.endswith((\".xls\", \".xlsx\")):\n",
        "        df = pd.read_excel(file_path, nrows=1, engine=\"openpyxl\")  # Read only header\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide a CSV or Excel file.\")\n",
        "\n",
        "    # Get actual column names\n",
        "    all_columns = df.columns.tolist()\n",
        "\n",
        "\n",
        "    des=[col for col in all_columns if any(re.search(keyword, col, re.IGNORECASE) for keyword in keywords)]\n",
        "    return des\n",
        "\n",
        "#-----------------------------------------------------------------\n",
        "#anonymizing descriptive values\n",
        "#-----------------------------------------------------------------\n",
        "def detect_noun_desc(text):\n",
        "    doc = nlp(text)\n",
        "    modified_text=[]\n",
        "    for token in doc:\n",
        "        if token.pos_ in ['PROPN']:\n",
        "            modified_text.append(\"<sensitive>\")\n",
        "        else:\n",
        "            modified_text.append(token.text)\n",
        "    text=\" \".join(modified_text)\n",
        "    return text\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------\n",
        "#function for detecting numerical sensitive\n",
        "#------------------------------------------------------------------\n",
        "def detect_sensitive_numerical(file_path,sensitive):\n",
        "    if file_path.endswith(\".csv\"):\n",
        "        df = pd.read_csv(file_path, engine=\"python\")\n",
        "    elif file_path.endswith((\".xls\", \".xlsx\")):\n",
        "        df = pd.read_excel(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide a CSV or Excel file.\")\n",
        "\n",
        "    sensitive_columns = []\n",
        "\n",
        "    for col in df.columns:\n",
        "        if col not in sensitive:\n",
        "          text_samples = df[col].astype(str).head(5)  # Convert first 5 values to string\n",
        "\n",
        "          for value in text_samples:\n",
        "              if pd.notna(value) and value.strip():\n",
        "                  results = analyzer.analyze(text=value, language=\"en\")\n",
        "\n",
        "                  for result in results:\n",
        "                      # print(result,result.entity_type)\n",
        "                      if result.entity_type in [\"PHONE_NUMBER\", \"CREDIT_CARD\", \"IBAN\", \"US_SSN\",\"EMAIL\"]:\n",
        "                          sensitive_columns.append(col)\n",
        "                          break  # If any value in the column is sensitive, mark the whole column\n",
        "\n",
        "    return list(set(sensitive_columns))\n",
        "#-------------------------------------------------------------------\n",
        "#function to anonymize a excel file\n",
        "#-------------------------------------------------------------------\n",
        "def excel_an(input_file, output_file):\n",
        "    df = pd.read_excel(input_file)  # Read as string for safety\n",
        "\n",
        "    sensitive_old = detect_noun(input_file)\n",
        "    desc = descriptive_columns(input_file)\n",
        "    sensitive = list(set(sensitive_old) - set(desc))\n",
        "    num_sensitive=detect_sensitive_numerical(input_file,sensitive_old)\n",
        "    sensitive=list(set(sensitive)+set(num_sensitive))\n",
        "\n",
        "    column_counters = {col: 1 for col in df.columns if col in sensitive_old}\n",
        "    column_mappings = {col: {} for col in sensitive}  # Store mappings for each column\n",
        "    mapping = {}\n",
        "\n",
        "    # Anonymize sensitive columns while maintaining consistency\n",
        "    for col in sensitive:\n",
        "        new_values = []\n",
        "        for val in df[col].astype(str):\n",
        "            if pd.notna(val):\n",
        "                if val in column_mappings[col]:\n",
        "                    anonymized_value = column_mappings[col][val]  # Use existing mapping\n",
        "                else:\n",
        "                    anonymized_value = f\"{col}{column_counters[col]}\"\n",
        "                    column_mappings[col][val] = anonymized_value  # Store new mapping\n",
        "                    column_counters[col] += 1  # Increment counter\n",
        "\n",
        "                mapping[anonymized_value] = val\n",
        "                new_values.append(anonymized_value)\n",
        "            else:\n",
        "                new_values.append(val)\n",
        "\n",
        "        df[col] = new_values\n",
        "\n",
        "    # Anonymize descriptive columns\n",
        "    for col in desc:\n",
        "        for idx, val in enumerate(df[col].astype(str)):\n",
        "            if pd.notna(val):\n",
        "                an_values = detect_noun_desc(val)\n",
        "                df.at[idx, col] = an_values\n",
        "                mapping[an_values] = val\n",
        "\n",
        "    df.to_excel(output_file, index=False, sheet_name=\"Anonymized Data\")\n",
        "    print(f\"✅ Anonymized file saved as {output_file}\")\n",
        "\n",
        "    # Save mapping as JSON\n",
        "    with open(\"mappings.json\", \"w\") as f:\n",
        "        json.dump(mapping, f)\n",
        "\n",
        "#------------------------------------------------------\n",
        "#function for de-anonymizing excel data\n",
        "#------------------------------------------------------\n",
        "\n",
        "def excel_dean(input_file, output_file, mapping_file):\n",
        "    print(\"🔄 Loading data...\")\n",
        "\n",
        "    with open(mapping_file, \"r\") as f:\n",
        "        mapping = json.load(f)\n",
        "\n",
        "    df = pd.read_excel(input_file)  # Read as string for safety\n",
        "    mapping_keys = set(mapping.keys())\n",
        "    df = df.applymap(lambda x: mapping[x] if x in mapping_keys else x)\n",
        "    df.to_excel(output_file, index=False, sheet_name=\"De-anonymized Data\")\n",
        "\n",
        "    print(f\"✅ De-anonymized file saved as {output_file}\")\n",
        "\n",
        "#----------------------------------------------------------\n",
        "#function for anonymizing csv data\n",
        "#----------------------------------------------------------\n",
        "def csv_an(input_file, output_file):\n",
        "    df = pd.read_csv(input_file, engine=\"python\")\n",
        "\n",
        "    sensitive_old = detect_noun(input_file)\n",
        "    desc = descriptive_columns(input_file)\n",
        "    sensitive = list(set(sensitive_old) - set(desc))\n",
        "    num_sensitive=detect_sensitive_numerical(input_file,sensitive_old)\n",
        "    sensitive=sensitive+num_sensitive\n",
        "    print(sensitive)\n",
        "\n",
        "    # column_counters = {col: 1 for col in df.columns if col in sensitive}\n",
        "    # column_mappings = {col: {} for col in sensitive}  # Store mappings for each column\n",
        "\n",
        "    # # Anonymize sensitive columns while maintaining consistency\n",
        "    # for col in sensitive:\n",
        "    #     new_values = []\n",
        "    #     for val in df[col].astype(str):\n",
        "    #         if pd.notna(val):\n",
        "    #             if val in column_mappings[col]:\n",
        "    #                 anonymized_value = column_mappings[col][val]  # Use existing mapping\n",
        "    #             else:\n",
        "    #                 anonymized_value = f\"{col}{column_counters[col]}\"\n",
        "    #                 column_mappings[col][val] = anonymized_value  # Store new mapping\n",
        "    #                 column_counters[col] += 1  # Increment counter\n",
        "\n",
        "    #             mapping[anonymized_value] = val\n",
        "    #             new_values.append(anonymized_value)\n",
        "    #         else:\n",
        "    #             new_values.append(val)\n",
        "\n",
        "    #     df[col] = new_values\n",
        "\n",
        "\n",
        "    column_mappings = {col: {} for col in sensitive}  # Store mappings for each column\n",
        "\n",
        "    # Anonymize sensitive columns while maintaining consistency\n",
        "    for col in sensitive:\n",
        "        if col in df.columns:  # Ensure column exists\n",
        "            # Convert to string only once for entire column\n",
        "            str_col = df[col].astype(str)\n",
        "\n",
        "            # Create mask for non-NA values\n",
        "            mask = str_col.notna()\n",
        "\n",
        "            # Apply faking_data only to non-NA values\n",
        "            df[col] = str_col.where(~mask, str_col[mask].apply(anonymize_text))\n",
        "\n",
        "        # Anonymize descriptive columns\n",
        "    for col in desc:\n",
        "        for idx, val in enumerate(df[col].astype(str)):\n",
        "            if pd.notna(val):\n",
        "                an_values = detect_noun_desc(val)\n",
        "                df.at[idx, col] = an_values\n",
        "                mapping[an_values] = val\n",
        "\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"✅ Anonymized file saved as {output_file}\")\n",
        "\n",
        "    # Save mapping as JSON\n",
        "    with open(\"mappings.json\", \"w\") as f:\n",
        "        json.dump(mapping, f)\n",
        "\n",
        "#------------------------------------------------------------\n",
        "#function for de anonymizing csv data\n",
        "#------------------------------------------------------------\n",
        "def csv_dean(input_file, output_file, mapping_file):\n",
        "    print(\"🔄 Loading data...\")\n",
        "\n",
        "    with open(mapping_file, \"r\") as f:\n",
        "        mapping = json.load(f)\n",
        "    df = pd.read_csv(input_file, engine=\"python\", dtype=str)  # Read as string for safety\n",
        "    mapping_keys = set(mapping.keys())\n",
        "    df = df.applymap(lambda x: mapping[x] if x in mapping_keys else x)\n",
        "    df.to_csv(output_file, index=False)\n",
        "\n",
        "    print(f\"✅ De-anonymized file saved as {output_file}\")\n",
        "#------------------------------------------------------------\n",
        "#function for faker\n",
        "#------------------------------------------------------------\n",
        "from faker import Faker\n",
        "import re\n",
        "fake=Faker()\n",
        "PHONE_REGEX = r\"(\\+?\\d{1,3}[-.\\s]?)?\\(?\\d{1,4}\\)?[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,9}\"\n",
        "EMAIL_REGEX = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\"\n",
        "def anonymize_text(text):\n",
        "    results = analyzer.analyze(text=text, language=\"en\")\n",
        "\n",
        "    new_text = text\n",
        "    for result in sorted(results, key=lambda x: x.start, reverse=True):  # Process from the end to avoid shifting indexes\n",
        "        fake_value = \"\"\n",
        "        if result.entity_type == \"PERSON\":\n",
        "            fake_value = fake.name()\n",
        "        elif result.entity_type == \"GPE\":\n",
        "            fake_value = fake.city()\n",
        "        elif result.entity_type == \"ORG\":\n",
        "            fake_value = fake.company()\n",
        "\n",
        "\n",
        "        mapping[fake_value] = text[result.start:result.end]\n",
        "        new_text = new_text[:result.start] + fake_value + new_text[result.end:]\n",
        "\n",
        "    return new_text\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------\n",
        "#function to determine whcih type of data is need to perform\n",
        "#--------------------------------------------------------------\n",
        "def anonymization(input_file):\n",
        "    if input_file.endswith(\".csv\"):\n",
        "        csv_an(input_file,\"intermediate.csv\")\n",
        "        csv_dean(\"intermediate.csv\",\"deanonymized.csv\",\"mappings.json\")\n",
        "    elif input_file.endswith(\".xlsx\"):\n",
        "        excel_an(input_file,\"intermediate.xlsx\")\n",
        "        excel_dean(\"intermediate.xlsx\",\"deanonymized.xlsx\",\"mappings.json\")\n",
        "\n",
        "file=\"numerical.csv\"\n",
        "anonymization(file)"
      ],
      "metadata": {
        "id": "1bSCFFxznhj5",
        "outputId": "e715985a-a270-4282-b7fc-f28c50d660a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: pl, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNifRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNieRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItDriverLicenseRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItFiscalCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItVatCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItIdentityCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItPassportRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - PlPeselRecognizer supported languages: pl, registry supported languages: en\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Address', 'Name', 'Phone Number', 'Credit Card']\n",
            "✅ Anonymized file saved as intermediate.csv\n",
            "🔄 Loading data...\n",
            "✅ De-anonymized file saved as deanonymized.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-e6def0f03943>:261: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(lambda x: mapping[x] if x in mapping_keys else x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSo4-l-5Ul3v",
        "outputId": "e653e011-2d2c-4071-b125-020cf3b2032a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faker in /usr/local/lib/python3.11/dist-packages (37.0.2)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from faker import Faker\n",
        "import re\n",
        "fake=Faker()\n",
        "PHONE_REGEX = r\"(\\+?\\d{1,3}[-.\\s]?)?\\(?\\d{1,4}\\)?[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,9}\"\n",
        "EMAIL_REGEX = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\"\n",
        "def faking_data(text):\n",
        "  text=re.sub(PHONE_REGEX,fake.phone_number(),text)\n",
        "  text=re.sub(EMAIL_REGEX,fake.email(),text)\n",
        "  doc=nlp(text)\n",
        "  new_text=text\n",
        "  for ent in doc.ents:\n",
        "    fake_value=\"\"\n",
        "    if ent.label_==\"PERSON\":\n",
        "      fake_value=fake.name()\n",
        "    elif ent.label_==\"GPE\":\n",
        "      fake_value=fake.city()\n",
        "    elif ent.label_==\"ORG\":\n",
        "      fake_value=fake.company()\n",
        "    if fake_value:\n",
        "      new_text=new_text.replace(ent.text,fake_value)\n",
        "  return new_text\n"
      ],
      "metadata": {
        "id": "pHxfu7UwUuKR"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def csv_an(input_file, output_file):\n",
        "    df = pd.read_csv(input_file, engine=\"python\")\n",
        "\n",
        "    sensitive_old = detect_noun(input_file)\n",
        "    desc = descriptive_columns(input_file)\n",
        "    sensitive = list(set(sensitive_old) - set(desc))\n",
        "    num_sensitive=detect_sensitive_numerical(input_file,sensitive_old)\n",
        "    sensitive=sensitive+num_sensitive\n",
        "    # column_counters = {col: 1 for col in df.columns if col in sensitive}\n",
        "    column_mappings = {}  # Store mappings for each column\n",
        "\n",
        "    # Anonymize sensitive columns while maintaining consistency\n",
        "    for col in sensitive:\n",
        "        new_values = []\n",
        "        for val in df[col].astype(str):\n",
        "            if pd.notna(val):\n",
        "                if val in column_mappings[col]:\n",
        "                    anonymized_value = column_mappings[val]  # Use existing mapping\n",
        "                else:\n",
        "                    anonymized_value = faking_data(val)\n",
        "                    column_mappings[val] = anonymized_value  # Store new mapping\n",
        "                mapping[anonymized_value] = val\n",
        "                new_values.append(anonymized_value)\n",
        "            else:\n",
        "                new_values.append(val)\n",
        "\n",
        "        df[col] = new_values\n",
        "\n",
        "    # Anonymize descriptive columns\n",
        "    for col in desc:\n",
        "        for idx, val in enumerate(df[col].astype(str)):\n",
        "            if pd.notna(val):\n",
        "                an_values = detect_noun_desc(val)\n",
        "                df.at[idx, col] = an_values\n",
        "                mapping[an_values] = val\n",
        "\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"✅ Anonymized file saved as {output_file}\")\n",
        "\n",
        "    # Save mapping as JSON\n",
        "    with open(\"mappings.json\", \"w\") as f:\n",
        "        json.dump(mapping, f)"
      ],
      "metadata": {
        "id": "COaVl3OUXAUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mimesis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDhYiVpaRWRa",
        "outputId": "7df69dd5-3876-4610-c08e-afc7405d4664"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mimesis\n",
            "  Downloading mimesis-18.0.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading mimesis-18.0.0-py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mimesis\n",
            "Successfully installed mimesis-18.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(fake))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkzyiDnK2umR",
        "outputId": "b5efa8f5-50c6-49e0-b2aa-4642b22babf0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__annotations__', '__class__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_factories', '_factory_map', '_locales', '_map_provider_method', '_optional_proxy', '_select_factory', '_select_factory_choice', '_select_factory_distribution', '_unique_proxy', '_weights', 'aba', 'add_provider', 'address', 'administrative_unit', 'am_pm', 'android_platform_token', 'ascii_company_email', 'ascii_email', 'ascii_free_email', 'ascii_safe_email', 'bank_country', 'basic_phone_number', 'bban', 'binary', 'boolean', 'bothify', 'bs', 'building_number', 'cache_pattern', 'catch_phrase', 'century', 'chrome', 'city', 'city_prefix', 'city_suffix', 'color', 'color_hsl', 'color_hsv', 'color_name', 'color_rgb', 'color_rgb_float', 'company', 'company_email', 'company_suffix', 'coordinate', 'country', 'country_calling_code', 'country_code', 'credit_card_expire', 'credit_card_full', 'credit_card_number', 'credit_card_provider', 'credit_card_security_code', 'cryptocurrency', 'cryptocurrency_code', 'cryptocurrency_name', 'csv', 'currency', 'currency_code', 'currency_name', 'currency_symbol', 'current_country', 'current_country_code', 'date', 'date_between', 'date_between_dates', 'date_object', 'date_of_birth', 'date_this_century', 'date_this_decade', 'date_this_month', 'date_this_year', 'date_time', 'date_time_ad', 'date_time_between', 'date_time_between_dates', 'date_time_this_century', 'date_time_this_decade', 'date_time_this_month', 'date_time_this_year', 'day_of_month', 'day_of_week', 'del_arguments', 'dga', 'doi', 'domain_name', 'domain_word', 'dsv', 'ean', 'ean13', 'ean8', 'ein', 'email', 'emoji', 'enum', 'factories', 'file_extension', 'file_name', 'file_path', 'firefox', 'first_name', 'first_name_female', 'first_name_male', 'first_name_nonbinary', 'fixed_width', 'format', 'free_email', 'free_email_domain', 'future_date', 'future_datetime', 'generator_attrs', 'get_arguments', 'get_formatter', 'get_providers', 'get_words_list', 'hex_color', 'hexify', 'hostname', 'http_method', 'http_status_code', 'iana_id', 'iban', 'image', 'image_url', 'internet_explorer', 'invalid_ssn', 'ios_platform_token', 'ipv4', 'ipv4_network_class', 'ipv4_private', 'ipv4_public', 'ipv6', 'isbn10', 'isbn13', 'iso8601', 'items', 'itin', 'job', 'job_female', 'job_male', 'json', 'json_bytes', 'language_code', 'language_name', 'last_name', 'last_name_female', 'last_name_male', 'last_name_nonbinary', 'latitude', 'latlng', 'lexify', 'license_plate', 'linux_platform_token', 'linux_processor', 'local_latlng', 'locale', 'locales', 'localized_ean', 'localized_ean13', 'localized_ean8', 'location_on_land', 'longitude', 'mac_address', 'mac_platform_token', 'mac_processor', 'md5', 'military_apo', 'military_dpo', 'military_ship', 'military_state', 'mime_type', 'month', 'month_name', 'msisdn', 'name', 'name_female', 'name_male', 'name_nonbinary', 'nic_handle', 'nic_handles', 'null_boolean', 'numerify', 'opera', 'optional', 'paragraph', 'paragraphs', 'parse', 'passport_dates', 'passport_dob', 'passport_full', 'passport_gender', 'passport_number', 'passport_owner', 'password', 'past_date', 'past_datetime', 'phone_number', 'port_number', 'postalcode', 'postalcode_in_state', 'postalcode_plus4', 'postcode', 'postcode_in_state', 'prefix', 'prefix_female', 'prefix_male', 'prefix_nonbinary', 'pricetag', 'profile', 'provider', 'providers', 'psv', 'pybool', 'pydecimal', 'pydict', 'pyfloat', 'pyint', 'pyiterable', 'pylist', 'pyobject', 'pyset', 'pystr', 'pystr_format', 'pystruct', 'pytimezone', 'pytuple', 'random', 'random_choices', 'random_digit', 'random_digit_above_two', 'random_digit_not_null', 'random_digit_not_null_or_empty', 'random_digit_or_empty', 'random_element', 'random_elements', 'random_int', 'random_letter', 'random_letters', 'random_lowercase_letter', 'random_number', 'random_sample', 'random_uppercase_letter', 'randomize_nb_elements', 'rgb_color', 'rgb_css_color', 'ripe_id', 'safari', 'safe_color_name', 'safe_domain_name', 'safe_email', 'safe_hex_color', 'sbn9', 'secondary_address', 'seed', 'seed_instance', 'seed_locale', 'sentence', 'sentences', 'set_arguments', 'set_formatter', 'sha1', 'sha256', 'simple_profile', 'slug', 'ssn', 'state', 'state_abbr', 'street_address', 'street_name', 'street_suffix', 'suffix', 'suffix_female', 'suffix_male', 'suffix_nonbinary', 'swift', 'swift11', 'swift8', 'tar', 'text', 'texts', 'time', 'time_delta', 'time_object', 'time_series', 'timezone', 'tld', 'tsv', 'unique', 'unix_device', 'unix_partition', 'unix_time', 'upc_a', 'upc_e', 'uri', 'uri_extension', 'uri_page', 'uri_path', 'url', 'user_agent', 'user_name', 'uuid4', 'vin', 'weights', 'windows_platform_token', 'word', 'words', 'xml', 'year', 'zip', 'zipcode', 'zipcode_in_state', 'zipcode_plus4']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing columnwise replacing using Faker\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import pandas as pd\n",
        "from faker import Faker\n",
        "from presidio_analyzer import AnalyzerEngine\n",
        "\n",
        "# Initialize Faker & Presidio Analyzer\n",
        "fake = Faker()\n",
        "Faker.seed(42)\n",
        "analyzer = AnalyzerEngine()\n",
        "\n",
        "# Mapping file for storing forward/reverse mappings\n",
        "MAPPING_FILE = \"mappings.json\"\n",
        "forward_mapping = {}\n",
        "reverse_mapping = {}\n",
        "\n",
        "def time_it(func):\n",
        "    \"\"\"Decorator to measure execution time.\"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end = time.time()\n",
        "        print(f'\\nExecution time {func.__name__}: {end-start:.6f} seconds')\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "def load_mapping():\n",
        "    \"\"\"Loads existing mappings from a file if available.\"\"\"\n",
        "    global forward_mapping, reverse_mapping\n",
        "    if os.path.exists(MAPPING_FILE):\n",
        "        try:\n",
        "            with open(MAPPING_FILE, \"r\") as f:\n",
        "                data = json.load(f)\n",
        "                forward_mapping = data.get(\"forward_mapping\", {})\n",
        "                reverse_mapping = data.get(\"reverse_mapping\", {})\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Error decoding JSON file: {e}\")\n",
        "            forward_mapping = {}\n",
        "            reverse_mapping = {}\n",
        "\n",
        "def save_mapping():\n",
        "    \"\"\"Saves the mapping to a JSON file.\"\"\"\n",
        "    with open(MAPPING_FILE, \"w\") as f:\n",
        "        json.dump({\"forward_mapping\": forward_mapping, \"reverse_mapping\": reverse_mapping}, f, indent=4)\n",
        "\n",
        "@time_it\n",
        "def analyze_column(df):\n",
        "    \"\"\"Detects sensitive columns using Presidio.\"\"\"\n",
        "    entity_columns = {}\n",
        "    for col in df.columns:\n",
        "        unique_values = df[col].dropna().astype(str).unique()[:15]\n",
        "        entity_counts = {}\n",
        "\n",
        "        for value in unique_values:\n",
        "            results = analyzer.analyze(text=value, language=\"en\")\n",
        "            for result in results:\n",
        "                entity_counts[result.entity_type] = entity_counts.get(result.entity_type, 0) + 1\n",
        "\n",
        "        if entity_counts:\n",
        "            predominant_entity = max(entity_counts, key=entity_counts.get)\n",
        "            if predominant_entity not in entity_columns:\n",
        "                entity_columns[predominant_entity] = []\n",
        "            entity_columns[predominant_entity].append(col)\n",
        "\n",
        "    return entity_columns\n",
        "\n",
        "def load_and_analyze(file_path):\n",
        "    \"\"\"Loads CSV/Excel and analyzes sensitive columns.\"\"\"\n",
        "    df = pd.read_csv(file_path) if file_path.endswith(\".csv\") else pd.read_excel(file_path)\n",
        "    return analyze_column(df)\n",
        "\n",
        "def generate_fake_values(df, entity_type, column):\n",
        "    \"\"\"Generates fake values for an entire column while preserving uniqueness.\"\"\"\n",
        "    faker_mapping = {\n",
        "        \"PERSON\": fake.name,\n",
        "        \"FIRST_NAME\": fake.first_name,\n",
        "        \"LAST_NAME\": fake.last_name,\n",
        "        \"EMAIL\": fake.email,\n",
        "        \"URL\": fake.url,\n",
        "        \"PHONE_NUMBER\": fake.phone_number,\n",
        "        \"CREDIT_CARD\": fake.credit_card_number,\n",
        "        \"IBAN\": fake.iban,\n",
        "        \"US_SSN\": fake.ssn,\n",
        "        \"LOCATION\": fake.company,\n",
        "        \"STREET_ADDRESS\": fake.street_address,\n",
        "        \"CITY\": fake.city,\n",
        "        \"STATE\": fake.state,\n",
        "        \"COUNTRY\": fake.country,\n",
        "        \"ZIP_CODE\": fake.zipcode,\n",
        "        \"ORGANIZATION\": fake.company,\n",
        "        \"JOB_TITLE\": fake.job,\n",
        "        \"USERNAME\": fake.user_name,\n",
        "        \"PASSWORD\": fake.password,\n",
        "        \"IP_ADDRESS\": fake.ipv4,\n",
        "        \"MAC_ADDRESS\": fake.mac_address,\n",
        "        \"LICENSE_PLATE\": fake.license_plate,\n",
        "        \"UUID\": fake.uuid4,\n",
        "        \"BANK_ACCOUNT\": fake.bban,\n",
        "        \"TRANSACTION_ID\": fake.uuid4,\n",
        "        \"DEVICE_ID\": fake.uuid4,\n",
        "        \"ID\": fake.uuid4,\n",
        "    }\n",
        "\n",
        "    if entity_type not in faker_mapping:\n",
        "        return df  # Skip if no Faker function exists\n",
        "\n",
        "    faker_func = faker_mapping[entity_type]\n",
        "\n",
        "    # Replace the entire column in one go with mapped fake values\n",
        "    original_values = df[column].astype(str).dropna().unique()\n",
        "    fake_values = [forward_mapping.get(value, faker_func()) for value in original_values]\n",
        "\n",
        "    # Store mappings\n",
        "    for original, fake_val in zip(original_values, fake_values):\n",
        "        forward_mapping[original] = fake_val\n",
        "        reverse_mapping[fake_val] = original\n",
        "\n",
        "    df[column] = df[column].map(forward_mapping).fillna(df[column])\n",
        "    return df\n",
        "\n",
        "@time_it\n",
        "def mask_sensitive_data(df, classified_columns):\n",
        "    \"\"\"Masks all detected sensitive columns using Faker.\"\"\"\n",
        "    for entity_type, columns in classified_columns.items():\n",
        "        for column in columns:\n",
        "            if column in df.columns:\n",
        "                df = generate_fake_values(df, entity_type, column)\n",
        "\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    load_mapping()\n",
        "    input_file = \"smaller_companies.csv\"\n",
        "    output_file = \"anonymized.csv\"\n",
        "\n",
        "    df = pd.read_csv(input_file)\n",
        "    classified_columns = load_and_analyze(input_file)\n",
        "\n",
        "    df = mask_sensitive_data(df, classified_columns)\n",
        "    df.to_csv(output_file, index=False)\n",
        "\n",
        "    save_mapping()\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "8RmBGwGPjSjg",
        "outputId": "76cabca5-5282-46b4-c373-05438ab17df4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: pl, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNifRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNieRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItDriverLicenseRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItFiscalCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItVatCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItIdentityCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItPassportRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - PlPeselRecognizer supported languages: pl, registry supported languages: en\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Sensitive cols: ['name', 'industry', 'locality', 'country', 'linkedin url']\n",
            "\n",
            "\n",
            "PII Columns: []\n",
            "✅ Anonymized file saved as anonymized.csv\n",
            "\n",
            "\n",
            "Execution time anonymization: 7.634835 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-39-4f5d48a6ae44>:179: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(lambda x: mapping.get(x, x))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ De-anonymized file saved as deanonymized.csv\n",
            "\n",
            "\n",
            "Execution time de_anonymization: 0.533211 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizing Thejus's logic\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from faker import Faker\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "from presidio_analyzer import AnalyzerEngine\n",
        "\n",
        "# Initialize Faker & Presidio Analyzer\n",
        "fake = Faker()\n",
        "Faker.seed(42)\n",
        "analyzer = AnalyzerEngine()\n",
        "\n",
        "# Mapping file for storing forward/reverse mappings\n",
        "MAPPING_FILE = \"mappings.json\"\n",
        "forward_mapping = {}\n",
        "reverse_mapping = {}\n",
        "\n",
        "NUMERIC_PATTERN = re.compile(r\"^[<>]?[\\d,.%]+$\")\n",
        "SENSITIVE_HEADERS = {\"phone\", \"mobile\", \"credit\", \"card\", \"id\"}\n",
        "EXCLUDED_ENTITIES = {\"DATE_TIME\"}  # Exclude unwanted entities\n",
        "\n",
        "\n",
        "def time_it(func):\n",
        "    \"\"\"Decorator to measure execution time of functions.\"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end = time.time()\n",
        "        print(f'\\n⏳ Execution time {func.__name__}: {end-start:.6f} seconds')\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "def load_mapping():\n",
        "    \"\"\"Loads existing mapping from file, ensuring proper data handling.\"\"\"\n",
        "    global forward_mapping, reverse_mapping\n",
        "    if os.path.exists(MAPPING_FILE):\n",
        "        try:\n",
        "            with open(MAPPING_FILE, \"r\") as f:\n",
        "                data = json.load(f)\n",
        "                forward_mapping = data.get(\"forward_mapping\", {})\n",
        "                reverse_mapping = data.get(\"reverse_mapping\", {})\n",
        "            print(\"📂 Existing mapping loaded.\")\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"⚠️ Error decoding JSON file: {e}\")\n",
        "\n",
        "\n",
        "def save_mapping():\n",
        "    \"\"\"Saves the mapping to a JSON file with string keys.\"\"\"\n",
        "    mapping_data = {\n",
        "        \"forward_mapping\": {str(k): str(v) for k, v in forward_mapping.items()},\n",
        "        \"reverse_mapping\": {str(k): str(v) for k, v in reverse_mapping.items()},\n",
        "        \"metadata\": {\n",
        "            \"updated_at\": datetime.now().isoformat(),\n",
        "            \"record_count\": len(forward_mapping),\n",
        "        },\n",
        "    }\n",
        "    with open(MAPPING_FILE, \"w\") as f:\n",
        "        json.dump(mapping_data, f, indent=4)\n",
        "    print(\"💾 Mapping saved successfully.\")\n",
        "\n",
        "\n",
        "\n",
        "@time_it\n",
        "def analyze_column(df):\n",
        "    \"\"\"Uses Presidio to classify columns based on detected entity types.\"\"\"\n",
        "    entity_columns = {}\n",
        "    for col in df.columns:\n",
        "        col_lower = col.lower()\n",
        "        if any(keyword in col_lower for keyword in SENSITIVE_HEADERS):\n",
        "            entity_columns.setdefault(\"SENSITIVE\", []).append(col)\n",
        "            continue\n",
        "\n",
        "        unique_values = df[col].dropna().astype(str).unique()[:10]\n",
        "        entity_counts = {}\n",
        "\n",
        "        for value in unique_values:\n",
        "            if NUMERIC_PATTERN.match(value):\n",
        "                continue  # Skip purely numeric values unless it's a phone/credit card\n",
        "\n",
        "            results = analyzer.analyze(text=value, language=\"en\")\n",
        "            for result in results:\n",
        "                if result.entity_type in EXCLUDED_ENTITIES:\n",
        "                    continue  # Skip unwanted entities\n",
        "                entity_counts[result.entity_type] = entity_counts.get(result.entity_type, 0) + 1\n",
        "\n",
        "        if entity_counts:\n",
        "            predominant_entity = max(entity_counts, key=entity_counts.get)\n",
        "            if predominant_entity not in entity_columns:\n",
        "                entity_columns[predominant_entity] = []\n",
        "            entity_columns[predominant_entity].append(col)\n",
        "\n",
        "    return entity_columns\n",
        "\n",
        "\n",
        "def generate_fake_values(column):\n",
        "    \"\"\"Generates consistent fake values for a column while preserving uniqueness.\"\"\"\n",
        "    col_name = column.name.lower()\n",
        "    original_values = column.dropna().unique()\n",
        "\n",
        "    if col_name in forward_mapping:\n",
        "        mapping = forward_mapping[col_name]\n",
        "    else:\n",
        "        mapping = {}\n",
        "\n",
        "    fake_values = []\n",
        "    if re.search(r\"\\b(phone|mobile)\\b\", col_name, re.IGNORECASE):\n",
        "        fake_values = [fake.phone_number() for _ in original_values]\n",
        "    elif re.search(r\"\\b(credit|card)\\b\", col_name, re.IGNORECASE):\n",
        "        fake_values = [fake.credit_card_number() for _ in original_values]\n",
        "    elif re.search(r\"\\b(id|identifier)\\b\", col_name, re.IGNORECASE):\n",
        "        fake_values = [fake.bothify(text=\"ID##########????\").upper() for _ in original_values]\n",
        "    elif re.search(r\"\\b(name|full[_\\s]?name|first[_\\s]?name|last[_\\s]?name)\\b\", col_name, re.IGNORECASE):\n",
        "        fake_values = [f'{fake.bothify(text=\"???#####\")}--{fake.name()}' for _ in original_values]\n",
        "    else:\n",
        "        fake_values = [fake.bothify(text=\"????########\").upper() for _ in original_values]\n",
        "\n",
        "\n",
        "    mapping.update(dict(zip(original_values, fake_values)))\n",
        "    forward_mapping[col_name] = mapping\n",
        "    reverse_mapping[col_name] = {v: k for k, v in mapping.items()}\n",
        "\n",
        "    return column.map(mapping).fillna(column)\n",
        "\n",
        "\n",
        "@time_it\n",
        "def mask_columnwise(df):\n",
        "    \"\"\"Replaces sensitive data only in columns detected as sensitive by Presidio.\"\"\"\n",
        "    sensitive_columns = analyze_column(df)\n",
        "\n",
        "    for entity, cols in sensitive_columns.items():\n",
        "        for column in cols:\n",
        "            print(f\"🔒 Masking sensitive column: {column} (Detected as {entity})\")\n",
        "            df[column] = generate_fake_values(df[column])\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def restore_data(df):\n",
        "    \"\"\"Restores original values using stored mappings.\"\"\"\n",
        "    for column in df.columns:\n",
        "        col_name = column.lower()\n",
        "        if col_name in reverse_mapping:\n",
        "            df[column] = df[column].astype(str).map(reverse_mapping[col_name]).fillna(df[column])\n",
        "    return df\n",
        "\n",
        "\n",
        "def check_if_files_match(original_file, restored_file):\n",
        "    \"\"\"Checks if the original and restored files match.\"\"\"\n",
        "    file_ext = os.path.splitext(original_file)[-1].lower()\n",
        "    original_df = pd.read_csv(original_file) if file_ext == \".csv\" else pd.read_excel(original_file)\n",
        "    restored_df = pd.read_csv(restored_file) if file_ext == \".csv\" else pd.read_excel(restored_file)\n",
        "    test = original_df.equals(restored_df)\n",
        "    print(f\"📊 Are files identical? {test}\")\n",
        "    if not test:\n",
        "        print(\"⚠️ Files are not identical!\")\n",
        "    else:\n",
        "        print(\"✅ Files match perfectly!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    load_mapping()\n",
        "    # uploaded = files.upload()\n",
        "    # input_file = list(uploaded.keys())[0].strip().replace(' ', '-')\n",
        "    input_file = \"ust.csv\"\n",
        "\n",
        "    anonymized_file = \"anonymized.csv\"\n",
        "    restored_file = \"restored.csv\"\n",
        "\n",
        "    df = pd.read_csv(input_file)\n",
        "    df = mask_columnwise(df)  # Only masks detected sensitive columns\n",
        "    df.to_csv(anonymized_file, index=False)\n",
        "    save_mapping()\n",
        "\n",
        "    df = restore_data(pd.read_csv(anonymized_file))\n",
        "    df.to_csv(restored_file, index=False)\n",
        "\n",
        "    check_if_files_match(input_file, restored_file)\n",
        ""
      ],
      "metadata": {
        "id": "nE16foVOwUNj",
        "outputId": "706f43ee-d759-4ff3-e668-dd5b72ba75cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: pl, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNifRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNieRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItDriverLicenseRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItFiscalCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItVatCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItIdentityCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItPassportRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - PlPeselRecognizer supported languages: pl, registry supported languages: en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    load_mapping()\n",
        "    # uploaded = files.upload()\n",
        "    # input_file = list(uploaded.keys())[0].strip().replace(' ', '-')\n",
        "    input_file = \"smaller_100k_companies.csv\"\n",
        "\n",
        "    anonymized_file = \"anonymized.csv\"\n",
        "    restored_file = \"restored.csv\"\n",
        "\n",
        "    df = pd.read_csv(input_file)\n",
        "    df = mask_columnwise(df)  # Only masks detected sensitive columns\n",
        "    df.to_csv(anonymized_file, index=False)\n",
        "    save_mapping()\n",
        "\n",
        "    df = restore_data(pd.read_csv(anonymized_file))\n",
        "    df.to_csv(restored_file, index=False)\n",
        "\n",
        "    check_if_files_match(input_file, restored_file)"
      ],
      "metadata": {
        "id": "Tjqnq9YvxEhE",
        "outputId": "8607f574-4f67-4bc4-b8f8-bcffcb0add2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "⏳ Execution time analyze_column: 0.789924 seconds\n",
            "🔒 Masking sensitive column: id (Detected as SENSITIVE)\n",
            "🔒 Masking sensitive column: name (Detected as LOCATION)\n",
            "🔒 Masking sensitive column: locality (Detected as LOCATION)\n",
            "🔒 Masking sensitive column: country (Detected as LOCATION)\n",
            "🔒 Masking sensitive column: domain (Detected as URL)\n",
            "🔒 Masking sensitive column: linkedin url (Detected as URL)\n",
            "\n",
            "⏳ Execution time mask_columnwise: 24.072139 seconds\n",
            "💾 Mapping saved successfully.\n",
            "📊 Are files identical? True\n",
            "✅ Files match perfectly!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "from faker import Faker\n",
        "from datetime import datetime\n",
        "from presidio_analyzer import AnalyzerEngine\n",
        "\n",
        "# Initialize Faker & Presidio Analyzer\n",
        "fake = Faker()\n",
        "Faker.seed(42)\n",
        "analyzer = AnalyzerEngine()\n",
        "\n",
        "# Mapping file for storing forward/reverse mappings\n",
        "MAPPING_FILE = \"mappings.json\"\n",
        "forward_mapping = {}\n",
        "reverse_mapping = {}\n",
        "\n",
        "NUMERIC_PATTERN = re.compile(r\"^[<>]?[\\d,.%]+$\")\n",
        "SENSITIVE_HEADERS = {\"phone\", \"mobile\", \"credit\", \"card\", \"id\"}\n",
        "EXCLUDED_ENTITIES = {\"DATE_TIME\"}  # Exclude unwanted entities\n",
        "\n",
        "def load_mapping():\n",
        "    \"\"\"Loads existing mapping from file.\"\"\"\n",
        "    global forward_mapping, reverse_mapping\n",
        "    if os.path.exists(MAPPING_FILE):\n",
        "        try:\n",
        "            with open(MAPPING_FILE, \"r\") as f:\n",
        "                data = json.load(f)\n",
        "                forward_mapping = data.get(\"forward_mapping\", {})\n",
        "                reverse_mapping = data.get(\"reverse_mapping\", {})\n",
        "            print(\"📂 Existing mapping loaded.\")\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"⚠️ Error decoding JSON file: {e}\")\n",
        "\n",
        "def save_mapping():\n",
        "    \"\"\"Saves the mapping to a JSON file.\"\"\"\n",
        "    mapping_data = {\n",
        "        \"forward_mapping\": {str(k): str(v) for k, v in forward_mapping.items()},\n",
        "        \"reverse_mapping\": {str(k): str(v) for k, v in reverse_mapping.items()},\n",
        "        \"metadata\": {\"updated_at\": datetime.now().isoformat(), \"record_count\": len(forward_mapping)},\n",
        "    }\n",
        "    with open(MAPPING_FILE, \"w\") as f:\n",
        "        json.dump(mapping_data, f, indent=4)\n",
        "    print(\"💾 Mapping saved successfully.\")\n",
        "\n",
        "def analyze_column(df):\n",
        "    \"\"\"Uses Presidio to classify columns based on detected entity types.\"\"\"\n",
        "    entity_columns = {}\n",
        "    for col in df.columns:\n",
        "        col_lower = col.lower()\n",
        "        if any(keyword in col_lower for keyword in SENSITIVE_HEADERS):\n",
        "            entity_columns.setdefault(\"SENSITIVE\", []).append(col)\n",
        "            continue\n",
        "\n",
        "        unique_values = df[col].dropna().astype(str).unique()[:10]\n",
        "        entity_counts = {}\n",
        "\n",
        "        for value in unique_values:\n",
        "            if NUMERIC_PATTERN.match(value):\n",
        "                continue  # Skip purely numeric values\n",
        "            results = analyzer.analyze(text=value, language=\"en\")\n",
        "            for result in results:\n",
        "                if result.entity_type in EXCLUDED_ENTITIES:\n",
        "                    continue  # Skip unwanted entities\n",
        "                entity_counts[result.entity_type] = entity_counts.get(result.entity_type, 0) + 1\n",
        "\n",
        "        if entity_counts:\n",
        "            predominant_entity = max(entity_counts, key=entity_counts.get)\n",
        "            entity_columns.setdefault(predominant_entity, []).append(col)\n",
        "    return entity_columns\n",
        "\n",
        "def get_user_selected_columns(df):\n",
        "    \"\"\"Prompts the user to select additional sensitive columns.\"\"\"\n",
        "    print(\"\\nColumns in the dataset:\")\n",
        "    for i, col in enumerate(df.columns):\n",
        "        print(f\"{i + 1}. {col}\")\n",
        "\n",
        "    selected_cols = input(\"Enter column numbers to mask (comma-separated): \")\n",
        "    selected_cols = [df.columns[int(i) - 1] for i in selected_cols.split(\",\") if i.isdigit()]\n",
        "    print(\"User-selected sensitive columns:\", selected_cols)\n",
        "    return selected_cols\n",
        "\n",
        "def generate_fake_values(column):\n",
        "    \"\"\"Generates consistent fake values for a column while preserving uniqueness.\"\"\"\n",
        "    col_name = column.name.lower()\n",
        "    original_values = column.dropna().unique()\n",
        "\n",
        "    if col_name in forward_mapping:\n",
        "        mapping = forward_mapping[col_name]\n",
        "    else:\n",
        "        mapping = {}\n",
        "\n",
        "    fake_values = []\n",
        "    if \"phone\" in col_name or \"mobile\" in col_name:\n",
        "        fake_values = [fake.phone_number() for _ in original_values]\n",
        "    elif \"credit\" in col_name or \"card\" in col_name:\n",
        "        fake_values = [fake.credit_card_number() for _ in original_values]\n",
        "    elif \"id\" in col_name:\n",
        "        fake_values = [fake.bothify(text='??####').upper() for _ in original_values]\n",
        "    elif \"email\" in col_name:\n",
        "        fake_values = [fake.email() for _ in original_values]\n",
        "    elif \"url\" in col_name:\n",
        "        fake_values = [fake.url() for _ in original_values]\n",
        "    else:\n",
        "        fake_values = [fake.bothify(text=\"????########\").upper() for _ in original_values]\n",
        "\n",
        "    mapping.update(dict(zip(original_values, fake_values)))\n",
        "    forward_mapping[col_name] = mapping\n",
        "    reverse_mapping[col_name] = {v: k for k, v in mapping.items()}\n",
        "\n",
        "    return column.map(mapping).fillna(column)\n",
        "\n",
        "def mask_columnwise(df, user_selected_cols):\n",
        "    \"\"\"Replaces sensitive data in detected and user-selected columns.\"\"\"\n",
        "    sensitive_columns = analyze_column(df)\n",
        "    for col in user_selected_cols:\n",
        "        sensitive_columns.setdefault(\"MANUAL\", []).append(col)\n",
        "\n",
        "    for entity, cols in sensitive_columns.items():\n",
        "        for column in cols:\n",
        "            print(f\"🔒 Masking column: {column} (Detected as {entity})\")\n",
        "            df[column] = generate_fake_values(df[column])\n",
        "    return df\n",
        "\n",
        "def restore_data(df):\n",
        "    \"\"\"Restores original values using stored mappings.\"\"\"\n",
        "    for column in df.columns:\n",
        "        col_name = column.lower()\n",
        "        if col_name in reverse_mapping:\n",
        "            df[column] = df[column].astype(str).map(reverse_mapping[col_name]).fillna(df[column])\n",
        "    return df\n",
        "\n",
        "def check_if_files_match(original_file, restored_file):\n",
        "    \"\"\"Checks if the original and restored files match.\"\"\"\n",
        "    original_df = pd.read_csv(original_file)\n",
        "    restored_df = pd.read_csv(restored_file)\n",
        "    print(f\"📊 Are files identical? {original_df.equals(restored_df)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    load_mapping()\n",
        "    input_file = \"sample.csv\"\n",
        "    anonymized_file = \"anonymized.csv\"\n",
        "    restored_file = \"restored.csv\"\n",
        "\n",
        "    df = pd.read_csv(input_file)\n",
        "    user_selected_cols = get_user_selected_columns(df)\n",
        "    df = mask_columnwise(df, user_selected_cols)\n",
        "    df.to_csv(anonymized_file, index=False)\n",
        "    save_mapping()\n",
        "\n",
        "    df = restore_data(pd.read_csv(anonymized_file))\n",
        "    df.to_csv(restored_file, index=False)\n",
        "    check_if_files_match(input_file, restored_file)\n",
        "())\n",
        ""
      ],
      "metadata": {
        "id": "eZsSy9Q4VhVX"
      },
      "execution_count": 35,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}